{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95d8992-e05e-482a-8b33-5c6725650d43",
   "metadata": {},
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b654fe-c703-479f-a1af-4c68f0a862e3",
   "metadata": {},
   "source": [
    "This notebook take a list of LD file and a list of sumstat file and do salmon QC and susie RSS for each overlap LD block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890a3a0-355f-4a5d-b355-6d4313332408",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f163e7-fa1e-480f-8278-53ec6c7602e2",
   "metadata": {},
   "source": [
    "1. A tab delimated table describing the path where LD per region stored, can be generated using the ld_per_region_plink step of the genotype processing module.\n",
    "\n",
    "```\n",
    "#id     dir\n",
    "chr17_60570445_65149278 /mnt/vast/hpc/csg/molecular_phenotype_calling/LD/output_npz_2/1300_hg38_EUR_LD_blocks_npz_files/ROSMAP_NIA_WGS.leftnorm.filtered.filtered.chr17_60570445_65149278.flt16.npz\n",
    "```\n",
    "\n",
    "2. A tab delimated table describing  path where summary stat per chromosome stored, can be generated using the yml_generator module before the qced sumstat are generated.\n",
    "```\n",
    "hs3163@csglogin:/mnt/vast/hpc/csg/xqtl_workflow_testing/susie_rss$ cat /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt\n",
    "#chr    ADGWAS_Bellenguez_2022\n",
    "1       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.1/ADGWAS2022.chr1.sumstat.tsv\n",
    "2       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.2/ADGWAS2022.chr2.sumstat.tsv\n",
    "3       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.3/ADGWAS2022.chr3.sumstat.tsv\n",
    "4       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.4/ADGWAS2022.chr4.sumstat.tsv\n",
    "5       /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/ADGWAS_Bellenguez_2022.5/ADGWAS2022.chr5.sumstat.tsv\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d648fb-0b8b-415f-aef0-4fd2eed5bdb3",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecf95d-257e-4f1a-994b-dcdacb85e18f",
   "metadata": {},
   "source": [
    "1. A RDS file containing the output susie object, the name of all variants that went through the analysis, the z score , and the LD used for the analysis.\n",
    "2. A sumstat file with additional column containing the slalom results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a504f-574e-4df4-8af5-188c2f8e3374",
   "metadata": {},
   "source": [
    "## MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bbdf169-3266-4cc5-8c01-f1a1ec060a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-158593d50ce7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-158593d50ce7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sos run pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sos run pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\n",
    "    --LD_list test.ld.list \\\n",
    "    --sumstat_list /mnt/vast/hpc/csg/xqtl_workflow_testing/ADGWAS/data_intergration/ADGWAS2022/qced_sumstat_list.txt \\\n",
    "    --container containers/stephenslab.sif --impute --cwd output_impute_2 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fdad0-169d-4224-9e70-6d343275c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: container = \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 3\n",
    "parameter: cwd = path(\"output\")\n",
    "# getting the overlapped input\n",
    "parameter: LD_list = path\n",
    "parameter: sumstat_list = path\n",
    "import pandas as pd\n",
    "LD_list = pd.read_csv(LD_list, sep = \"\\t\")\n",
    "sumstat_list = pd.read_csv(sumstat_list, sep = \"\\t\")\n",
    "LD_list[\"#chr\"] = [x[0].replace(\"chr\", \"\") for x in  LD_list[\"#id\"].str.split(\"_\") ]\n",
    "sumstat_list[\"#chr\"] = [str(x).replace(\"chr\", \"\") for x in  sumstat_list[\"#chr\"] ]\n",
    "input_inv = LD_list.merge(sumstat_list)\n",
    "input_list = input_inv.iloc[:,[1,3]].values.tolist()\n",
    "parameter: lead_idx_choice = \"pvalue\"\n",
    "parameter: abf_prior_variance = 0.4\n",
    "parameter: nlog10p_dentist_s_threshold = 4\n",
    "parameter: r2_threshold = 0.6\n",
    "parameter: n = 0\n",
    "parameter: max_iter = 1000\n",
    "parameter: impute = True # Whether to impute the sumstat for all the snp in LD but not in sumstat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31441-f4f8-41cd-9b5e-cdc65d22e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "[SuSiE_RSS_1]\n",
    "parameter: L = 10\n",
    "parameter: max_L = 1000\n",
    "input: input_list, group_by = 2\n",
    "name = f'{_input[0]:b}'.split(\".\")[-3]\n",
    "output: f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.fit.rds',\n",
    "        f'{cwd:a}/{_input[1]:bn}.{name}.unisusie_rss.ss_qced.tsv'\n",
    "\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import rpy2\n",
    "    from backports import zoneinfo\n",
    "    import rpy2.robjects.numpy2ri as numpy2ri\n",
    "    from rpy2.robjects import pandas2ri\n",
    "    from rpy2.robjects.packages import SignatureTranslatedAnonymousPackage\n",
    "    import rpy2.robjects as ro\n",
    "    numpy2ri.activate()\n",
    "    pandas2ri.activate()\n",
    "    def load_npz_ld(path):\n",
    "        np_ld_loaded = np.load(path,allow_pickle=True)\n",
    "        # sort by start position\n",
    "        snp_id = [x.replace(\":\",\"_\") for x in np_ld_loaded.get(\"arr_1\")]\n",
    "        np_ld_loaded = np_ld_loaded.get(\"arr_0\")\n",
    "        new = np_ld_loaded + np_ld_loaded.T\n",
    "        np.fill_diagonal(new, np.diag(new)/2)\n",
    "        return new,snp_id\n",
    "\n",
    "    def get_bcor_meta(bcor_obj):\n",
    "        df_ld_snps = bcor_obj.getMeta()\n",
    "        df_ld_snps.rename(columns={'rsid':'SNP', 'position':'BP', 'chromosome':'CHR', 'allele1':'A1', 'allele2':'A2'}, inplace=True, errors='raise')\n",
    "        ###df_ld_snps['CHR'] = df_ld_snps['CHR'].astype(np.int64)\n",
    "        df_ld_snps['BP'] = df_ld_snps['BP'].astype(np.int64)\n",
    "        return df_ld_snps\n",
    "\n",
    "    def load_ld_bcor(ld_prefix):\n",
    "        bcor_file = ld_prefix+'.bcor'\n",
    "        import os\n",
    "        import time\n",
    "        from ldstore.bcor import bcor\n",
    "        if not os.path.exists(bcor_file):\n",
    "            raise IOError('%s not found'%(bcor_file))\n",
    "        t0 = time.time()\n",
    "        bcor_obj = bcor(bcor_file)\n",
    "        df_ld_snps = get_bcor_meta(bcor_obj)\n",
    "        ld_arr = bcor_obj.readCorr([])\n",
    "        assert np.all(~np.isnan(ld_arr))\n",
    "        return ld_arr, df_ld_snps\n",
    "\n",
    "    def abf(beta, se, W=0.04):\n",
    "        from scipy import special \n",
    "        z = beta / se\n",
    "        V = se ** 2\n",
    "        r = W / (W + V)\n",
    "        lbf = 0.5 * (np.log(1 - r) + (r * z ** 2))\n",
    "        denom = special.logsumexp(lbf)\n",
    "        prob = np.exp(lbf - denom)\n",
    "        return lbf, prob\n",
    "    \n",
    "    def get_cs(variant, prob, coverage=0.95):\n",
    "        ordering = np.argsort(prob)[::-1]\n",
    "        idx = np.where(np.cumsum(prob[ordering]) > coverage)[0][0]\n",
    "        cs = variant[ordering][: (idx + 1)]\n",
    "        return cs\n",
    "    def slalom(df,LD,abf_prior_variance = 0.4 ,nlog10p_dentist_s_threshold = 4, r2_threshold = 0.6  ):\n",
    "        from scipy import stats\n",
    "        lbf, prob = abf(df.beta, df.se, W=abf_prior_variance)\n",
    "        cs = get_cs(df.variant, prob, coverage=0.95)\n",
    "        cs_99 = get_cs(df.variant, prob, coverage=0.99)\n",
    "        df[\"lbf\"] = lbf\n",
    "        df[\"prob\"] = prob\n",
    "        df[\"cs\"] = df.variant.isin(cs)\n",
    "        df[\"cs_99\"] = df.variant.isin(cs_99)\n",
    "        \n",
    "        if \"${lead_idx_choice}\" == \"pvalue\":\n",
    "            lead_idx_snp = df.pvalue.idxmin()\n",
    "        else:\n",
    "            lead_idx_snp = df.prob.idxmax()\n",
    "            \n",
    "        lead_variant = df.variant[lead_idx_snp]\n",
    "        df[\"lead_variant\"] = False\n",
    "        df[\"lead_variant\"].iloc[lead_idx_snp] = True\n",
    "        # annotate LD     \n",
    "        ## This is to identify the R for each snp vs the lead snp\n",
    "        df[\"r\"] = [LD[np.where(np.in1d(df.variant,lead_variant))][:,np.where(np.in1d(df.variant,x))][0][0][0] for x in df.variant]\n",
    "        lead_z = (df.beta / df.se).iloc[lead_idx_snp]\n",
    "        df[\"t_dentist_s\"] = ((df.beta / df.se) - df.r * lead_z) ** 2 / (1 - df.r ** 2)\n",
    "        df[\"t_dentist_s\"] = np.where(df[\"t_dentist_s\"] < 0, np.inf, df[\"t_dentist_s\"])\n",
    "        df[\"t_dentist_s\"].iloc[lead_idx_snp] = np.nan\n",
    "        df[\"nlog10p_dentist_s\"] = stats.chi2.logsf(df[\"t_dentist_s\"], df=1) / -np.log(10)\n",
    "        df[\"r2\"] = df.r ** 2\n",
    "        df[\"outliers\"] = (df.r2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        df_output = df\n",
    "        n_r2 = np.sum(df.r2 > r2_threshold)\n",
    "        n_dentist_s_outlier = np.sum(\n",
    "            (df.r2 > r2_threshold) & (df.nlog10p_dentist_s > nlog10p_dentist_s_threshold)\n",
    "        )\n",
    "        max_pip_idx = df.prob.idxmax()\n",
    "        df_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"lead_pip_variant\": [df.variant.iloc[max_pip_idx]],\n",
    "                \"n_total\": [len(df.index)],\n",
    "                \"n_r2\": [n_r2],\n",
    "                \"n_dentist_s_outlier\": [n_dentist_s_outlier],\n",
    "                \"fraction\": [n_dentist_s_outlier / n_r2 if n_r2 > 0 else 0],\n",
    "                \"max_pip\": [np.max(df.prob)]\n",
    "            }\n",
    "            )\n",
    "        return df, df_summary\n",
    "    \n",
    "    ## Load LD\n",
    "    if \"${_input[0]}\".endswith(\"npz\"):\n",
    "        LD,snp_id = load_npz_ld(${_input[0]:r}) \n",
    "    if \"${_input[0]}\".endswith(\"bcor\"):\n",
    "        LD,snp_id = load_ld_bcor(${_input[0]:nr}) \n",
    "        \n",
    "    sumstat = pd.read_csv(${_input[1]:r}, sep = \"\\t\")\n",
    "    ## Remove nan in LD\n",
    "    where = np.where(np.isnan(LD))\n",
    "    na_snp = np.array(snp_id)[[x for x in where[0]] + [x for x in where[1]]]\n",
    "    ### Remove NA from LD\n",
    "    LD = LD[np.ix_(~np.in1d(snp_id, na_snp),~np.in1d(snp_id, na_snp))]\n",
    "    ### Remove NA from snp_list\n",
    "    snp_id = np.array(snp_id)[~np.in1d(snp_id, na_snp)]\n",
    "    \n",
    "    ## Get only intersect snp\n",
    "    intersct = np.intersect1d(sumstat.variant.to_numpy(),snp_id)\n",
    "    sumstat = sumstat.query(\"variant in @intersct\").reset_index()\n",
    "    indice = np.where(np.in1d(snp_id, intersct))\n",
    "    ## slalom\n",
    "    ss_qc,ss_qc_sum = slalom(sumstat,LD[np.ix_(indice[0].tolist(), indice[0].tolist())],${abf_prior_variance},${nlog10p_dentist_s_threshold},${r2_threshold})\n",
    "    print(ss_qc_sum)\n",
    "    ss_qc.to_csv(${_output[1]:r},\"\\t\",index = 0 )\n",
    "    ## Filter out outlier\n",
    "    ### Get outliers\n",
    "    outliers = ss_qc[ss_qc.outliers].variant\n",
    "    ### Remove outliers from LD\n",
    "    LD = LD[np.ix_(~np.in1d(snp_id, outliers),~np.in1d(snp_id, outliers))]\n",
    "    ### Remove outliers from snp_list\n",
    "    snp_id = np.array(snp_id)[~np.in1d(snp_id, outliers)]\n",
    "    ss_qc = ss_qc.assign(z = ss_qc.beta/ss_qc.se)\n",
    "    ss_qc = ss_qc[~ss_qc.outliers]\n",
    "\n",
    "    if ${impute}:\n",
    "        cur_LD = LD\n",
    "        cur_miss = ~np.in1d(snp_id, ss_qc.variant)\n",
    "        print(f'{np.mean(cur_miss)} of snps in the LD panel does not have sumstat. Their sumstat are imputed')\n",
    "        ## Sumstat imputation based on fusion code\n",
    "        cur_wgt = cur_LD[np.ix_(cur_miss, np.logical_not(cur_miss))].dot(np.linalg.inv(cur_LD[np.logical_not(cur_miss), np.logical_not(cur_miss)] + 0.1 * np.eye(np.sum(np.logical_not(cur_miss)))))\n",
    "        cur_impz  = cur_wgt @ ss_qc.z\n",
    "        cur_r2pred = np.diag(cur_wgt.dot(cur_LD[np.ix_(np.logical_not(cur_miss),np.logical_not(cur_miss))]).dot(cur_wgt.T))\n",
    "        imputed_z = cur_impz/np.sqrt(cur_r2pred)\n",
    "        working_ss = pd.concat([ss_qc[[\"variant\",\"z\"]], pd.DataFrame({\"z\" : imputed_z, \"variant\": np.array(snp_id)[cur_miss]})]).set_index(\"variant\").reindex(snp_id).reset_index()\n",
    "        import scipy.stats as st\n",
    "        working_ss[\"beta\"] = working_ss.z\n",
    "        working_ss[\"se\"] = 1\n",
    "        working_ss[\"pvalue\"] = (1 - st.norm.cdf(working_ss.z))*2\n",
    "        \n",
    "        \n",
    "        working_ss_qc, working_ss_qc_sum = slalom(working_ss,cur_LD,0.4,4,0.6)\n",
    "        print(working_ss_qc_sum)\n",
    "        working_ss_qc.to_csv('${_output[1]:nn}.imputed_ss_qced.tsv',\"\\t\",index = 0 )\n",
    "        imputed_outliers = working_ss_qc[working_ss_qc.outliers].variant\n",
    "        ### Remove outliers from LD\n",
    "        cur_LD = LD[np.ix_(~np.in1d(snp_id, imputed_outliers),~np.in1d(snp_id, imputed_outliers))]\n",
    "        ### Remove outliers from snp_list\n",
    "        snp_id = np.array(snp_id)[~np.in1d(snp_id, imputed_outliers)]\n",
    "        working_ss_qc = working_ss_qc[~working_ss_qc.outliers]\n",
    "        ss_qc = working_ss_qc\n",
    "        LD = cur_LD \n",
    "    ### For non-imputed LD, retained only the overlap snps. For imputed LD, this should do nothing as snp_id, ss_qc.variant .\n",
    "    indice = np.where(np.in1d(snp_id, ss_qc.variant))\n",
    "    LD = LD[np.ix_(indice[0].tolist(), indice[0].tolist())]\n",
    "    ## SuSiERSS\n",
    "    string=\"\"\"\n",
    "    susie_rss_analysis=function(ss_df, R, n, var_y, z_ld_weight = 0, estimate_residual_variance = FALSE, \n",
    "    prior_variance = 50, check_prior = TRUE, output_path,L = ${L} , max_iter  = ${max_iter} ){\n",
    "    condz = susieR::kriging_rss(as.matrix(ss_df$z),R ,n = ${n})\n",
    "    ggplot2::ggsave(plot = condz$plot,filename = \"${_output[0]}.kriging_rss.pdf\", device = \"pdf\")\n",
    "    res = susieR:::susie_rss(as.matrix(ss_df$z),as.matrix(R), n = ${n},var_y, z_ld_weight = 0, estimate_residual_variance = FALSE, \n",
    "    prior_variance = 50, check_prior = TRUE, max_iter = ${max_iter},verbose = TRUE,L = ${L}   )\n",
    "    res$variants = ss_df$variant\n",
    "    res$z = ss_df$z\n",
    "    res$corr = susieR:::get_cs_correlation(res, X = NULL, Xcorr = R, max = FALSE)\n",
    "    res$conditional_dist = condz$conditional_dist \n",
    "    rownames(res$corr) <- names(res$cs)\n",
    "    colnames(res$corr) <- names(res$cs)\n",
    "    if (length(res$sets$cs) > 1) {\n",
    "        index_combos = expand.grid(1:length(res$sets$cs),1:length(res$sets$cs))\n",
    "        in_common = apply(index_combos, 1, function(x) intersect(res$sets$cs[[x[1]]], res$sets$cs[[x[2]]]))\n",
    "        counts = unlist(lapply(in_common, length))\n",
    "        ovlp_mat = matrix(counts, ncol = length(res$sets$cs), byrow = T)\n",
    "        ovlp_mat[lower.tri(ovlp_mat)] = NA\n",
    "        rownames(ovlp_mat) = names(res$sets$cs)\n",
    "        colnames(ovlp_mat) = names(res$sets$cs)\n",
    "        print(ovlp_mat)\n",
    "        res$sets[[\"ovlp_mat\"]] = ovlp_mat\n",
    "    }\n",
    "    saveRDS(res,output_path)\n",
    "    return(res)\n",
    "    }\n",
    "    \"\"\"\n",
    "    susie_analysis = SignatureTranslatedAnonymousPackage(string, \"susie_analysis\")\n",
    "    analysis_result = susie_analysis.susie_rss_analysis(ss_df = ss_qc,R = LD,output_path=${_output[0]:r} ${f', n ={n}' if n > 2 else \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c57e5a-d5e2-4454-8f08-e0f080d27b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "[SuSiE_RSS_2]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95eea28-da4f-4608-8cdd-dcd8155a41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[SuSiE_RSS_3]\n",
    "sep = \"\" #'\\n\\n---\\n'\n",
    "input: group_by = 'all'\n",
    "output: analysis_summary = f'{cwd}/{sumstats_path:bnn}.analysis_summary.md', variants_csv = f'{cwd}/{sumstats_path:bnn}.variants.csv'\n",
    "python: container=container, expand = \"${ }\"\n",
    "\n",
    "    theme = '''---\n",
    "    theme: base-theme\n",
    "    style: |\n",
    "     p {\n",
    "       font-size: 24px;\n",
    "       height: 900px;\n",
    "       margin-top:1cm;\n",
    "      }\n",
    "      img {\n",
    "        height: 70%;\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "      }\n",
    "      body {\n",
    "       margin-top: auto;\n",
    "       margin-bottom: auto;\n",
    "       font-family: verdana;\n",
    "      }\n",
    "    ---    \n",
    "    '''\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # will load the rds file outputted in a previous step\n",
    "    def load_rds(filename, types=None):\n",
    "        import os\n",
    "        import pandas as pd, numpy as np\n",
    "        import rpy2.robjects as RO\n",
    "        import rpy2.robjects.vectors as RV\n",
    "        import rpy2.rinterface as RI\n",
    "        from rpy2.robjects import numpy2ri\n",
    "        numpy2ri.activate()\n",
    "        from rpy2.robjects import pandas2ri\n",
    "        pandas2ri.activate()\n",
    "        def load(data, types, rpy2_version=3):\n",
    "            if types is not None and not isinstance(data, types):\n",
    "                return np.array([])\n",
    "            # FIXME: I'm not sure if I should keep two versions here\n",
    "            # rpy2_version 2.9.X is more tedious but it handles BoolVector better\n",
    "            # rpy2 version 3.0.1 converts bool to integer directly without dealing with\n",
    "            # NA properly. It gives something like (0,1,-234235).\n",
    "            # Possibly the best thing to do is to open an issue for it to the developers.\n",
    "            if rpy2_version == 2:\n",
    "                # below works for rpy2 version 2.9.X\n",
    "                if isinstance(data, RI.RNULLType):\n",
    "                    res = None\n",
    "                elif isinstance(data, RV.BoolVector):\n",
    "                    data = RO.r['as.integer'](data)\n",
    "                    res = np.array(data, dtype=int)\n",
    "                    # Handle c(NA, NA) situation\n",
    "                    if np.sum(np.logical_and(res != 0, res != 1)):\n",
    "                        res = res.astype(float)\n",
    "                        res[res < 0] = np.nan\n",
    "                        res[res > 1] = np.nan\n",
    "                elif isinstance(data, RV.FactorVector):\n",
    "                    data = RO.r['as.character'](data)\n",
    "                    res = np.array(data, dtype=str)\n",
    "                elif isinstance(data, RV.IntVector):\n",
    "                    res = np.array(data, dtype=int)\n",
    "                elif isinstance(data, RV.FloatVector):\n",
    "                    res = np.array(data, dtype=float)\n",
    "                elif isinstance(data, RV.StrVector):\n",
    "                    res = np.array(data, dtype=str)\n",
    "                elif isinstance(data, RV.DataFrame):\n",
    "                    res = pd.DataFrame(data)\n",
    "                elif isinstance(data, RV.Matrix):\n",
    "                    res = np.matrix(data)\n",
    "                elif isinstance(data, RV.Array):\n",
    "                    res = np.array(data)\n",
    "                else:\n",
    "                    # I do not know what to do for this\n",
    "                    # But I do not want to throw an error either\n",
    "                    res = str(data)\n",
    "            else:\n",
    "                if isinstance(data, RI.NULLType):\n",
    "                    res = None\n",
    "                else:\n",
    "                    res = data\n",
    "            if isinstance(res, np.ndarray) and res.shape == (1, ):\n",
    "                res = res[0]\n",
    "            return res\n",
    "        def load_dict(res, data, types):\n",
    "            '''load data to res'''\n",
    "            names = data.names if not isinstance(data.names, RI.NULLType) else [\n",
    "                i + 1 for i in range(len(data))\n",
    "            ]\n",
    "            for name, value in zip(names, list(data)):\n",
    "                if isinstance(value, RV.ListVector):\n",
    "                    res[name] = {}\n",
    "                    res[name] = load_dict(res[name], value, types)\n",
    "                else:\n",
    "                    res[name] = load(value, types)\n",
    "            return res\n",
    "        #\n",
    "        if not os.path.isfile(filename):\n",
    "            raise IOError('Cannot find file ``{}``!'.format(filename))\n",
    "        rds = RO.r['readRDS'](filename)\n",
    "        if isinstance(rds, RV.ListVector):\n",
    "            res = load_dict({}, rds, types)\n",
    "        else:\n",
    "            res = load(rds, types)\n",
    "        return res\n",
    "    \n",
    "    def f7(seq):\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "\n",
    "\n",
    "    text = \"\"\n",
    "    sep = '\\n\\n---\\n'\n",
    "    \n",
    "    inp = \"${_input:r}\".split(\" \")\n",
    "    for i, each in enumerate(inp):\n",
    "        inp[i] = \".\".join(each.split(\".\")[:-1])\n",
    "\n",
    "    r = f7(\"${_input:bn}\".split(\" \"))\n",
    "    \n",
    "    num_csets = []\n",
    "    region_info = []\n",
    "    \n",
    "    # this will be a 2d array that stores information about each variant of interest in the phenotype\n",
    "    # this includes all the variants in a cs and all the variants past the cutoff\n",
    "    variant_info = []\n",
    "\n",
    "    for reg_i, each in enumerate(f7(inp)):\n",
    "    \n",
    "        rid = r[reg_i].split('.')[0]\n",
    "        \n",
    "        text_temp = \"\"\n",
    "        text_temp += \"#\\n\\n SuSiE RSS {region} \\n\".format(region=r[reg_i])\n",
    "        text_temp += \"![]({region}.png){sep} \\n \\n\".format(region=r[reg_i], sep=sep)\n",
    "\n",
    "        rd = load_rds(each[1:]+\".rds\")\n",
    "        \n",
    "        # find the number of cs in the current region\n",
    "        if rd[\"sets\"][\"cs\"] == None:\n",
    "            num_csets.append(0)\n",
    "        else:\n",
    "            num_csets.append(len(rd[\"sets\"][\"cs\"]))\n",
    "        print(num_csets)\n",
    "        \n",
    "        # this will store the indicies of all variants that cross the threshold\n",
    "        ind_p = []\n",
    "\n",
    "        pval = ${pip_cutoff}\n",
    "\n",
    "        for i, each in enumerate(rd[\"pip\"]):\n",
    "            if each >= pval:\n",
    "                ind_p.append(i)\n",
    "        sumvars = 0\n",
    "        \n",
    "        # if we have at least one cs in the current region\n",
    "        if num_csets[reg_i] > 0:\n",
    "            tbl_header = \"| chr number | pos at highest pip | ref | alt | region id | cs | highest pip |  \\n\"\n",
    "            tbl_header += \"| --- | --- | --- | --- | --- | --- | --- |  \\n\"\n",
    "\n",
    "            table = \"\"\n",
    "            \n",
    "            sumpips = 0\n",
    "            \n",
    "            for cset in rd[\"sets\"][\"cs\"].keys():\n",
    "                print(cset)\n",
    "                \n",
    "                # if we have many variants in the cs\n",
    "                if isinstance(rd[\"sets\"][\"cs\"][cset], np.ndarray):\n",
    "                    highestpip = 0\n",
    "                    poswhighestpip = -1\n",
    "                    for i in rd[\"sets\"][\"cs\"][cset]:\n",
    "                        i = i.item() - 1\n",
    "                        \n",
    "                        # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "                        if i in ind_p: ind_p.remove(i) \n",
    "                        \n",
    "                        # append variant info\n",
    "                        variant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                        \n",
    "                        if rd[\"pip\"][i] > highestpip:\n",
    "                            highestpip = rd[\"pip\"][i]\n",
    "                            poswhighestpip = i\n",
    "                            \n",
    "                        sumpips += rd[\"pip\"][i]\n",
    "                        sumvars += 1\n",
    "                        \n",
    "                    if poswhighestpip > -1:\n",
    "                        i = poswhighestpip\n",
    "                        table += \"| {chr} | {pos} | {ref} | {alt} | {rid} | {cs} | {pip:.2f} |  \\n\".format(chr=rd[\"chr\"][i], pos=rd[\"pos\"][i], ref=rd[\"ref\"][i], alt=rd[\"alt\"][i], rid=rid, cs=cset, pip=rd[\"pip\"][i])\n",
    "                \n",
    "                else: # if we have only one variant in the cs\n",
    "                    i =  rd[\"sets\"][\"cs\"][cset]\n",
    "                    i = i.item() - 1\n",
    "                    \n",
    "                    # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "                    if i in ind_p: ind_p.remove(i)\n",
    "                    \n",
    "                    # append variant info\n",
    "                    variant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, cset, rd[\"pip\"][i]] )\n",
    "                    \n",
    "                    table += \"| {chr} | {pos} | {ref} | {alt} | {rid} | {cs} | {pip:.2f} |  \\n\".format(chr=rd[\"chr\"][i], pos=rd[\"pos\"][i], ref=rd[\"ref\"][i], alt=rd[\"alt\"][i], rid=rid, cs=cset, pip=rd[\"pip\"][i])\n",
    "                    \n",
    "                    sumpips += rd[\"pip\"][i]\n",
    "                    sumvars += 1\n",
    "            \n",
    "\n",
    "            text_temp += \"- Total number of variants: {}\\n\".format(len(rd[\"pip\"]))\n",
    "            text_temp += \"- Expected number of causal variants: {:.2f}\\n\".format(sumpips)\n",
    "            text_temp += \"- Number of variants with PIP > {} and not in any CS: {}\\n\\n\".format(pval, len(ind_p))\n",
    "            text_temp += tbl_header + table + sep\n",
    "            \n",
    "            if num_csets[reg_i] > 1:\n",
    "                text_temp += \"#### CORR: Correlation between CS | OLAP: Overlap between CS\\n\"\n",
    "                \n",
    "                cs = list(rd[\"sets\"][\"cs\"].keys())\n",
    "\n",
    "                corrheader = \"|  |\"\n",
    "                corrbreak = \"| --- |\"\n",
    "\n",
    "                for i in cs:\n",
    "                    corrheader += \" CORR {} |\".format(i)\n",
    "                    corrbreak += \" --- |\"\n",
    "                    \n",
    "                corrheader += \"  |\"\n",
    "                corrbreak += \" --- |\"\n",
    "                    \n",
    "                for i in cs:\n",
    "                    corrheader += \" OLAP {} |\".format(i)\n",
    "                    corrbreak += \" --- |\"\n",
    "\n",
    "                corrheader += \"\\n\"\n",
    "                corrbreak += \"\\n\"\n",
    "\n",
    "                body = \"\"\n",
    "\n",
    "                for en, i in enumerate(cs):\n",
    "                    body += \"| {} |\".format(i)\n",
    "                    for j in rd[\"cscorr\"][en]:\n",
    "                        body += \" {:.2f} |\".format(j)\n",
    "                    body += \"  |\"\n",
    "                    for j in rd[\"sets\"][\"cs\"]:\n",
    "                        body += \" {} |\".format(len(np.intersect1d(rd[\"sets\"][\"cs\"][i], rd[\"sets\"][\"cs\"][j])))\n",
    "                    body += \"\\n\"\n",
    "                \n",
    "                text_temp += corrheader + corrbreak + body + sep\n",
    "            \n",
    "        region_info.append(text_temp)\n",
    "            \n",
    "    f = open(${_output[\"analysis_summary\"]:r}, \"w\")\n",
    "    \n",
    "    cset_order = np.argsort(num_csets)\n",
    "    cset_order = cset_order.tolist()\n",
    "    cset_order.reverse()\n",
    "    for c in cset_order:\n",
    "        text += region_info[c]\n",
    "    \n",
    "    f.write(theme + text)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    for i in ind_p:\n",
    "        # append variant info\n",
    "        variant_info.append( [rd[\"chr\"][i], rd[\"pos\"][i], rd[\"ref\"][i], rd[\"alt\"][i], rid, \"None\", rd[\"pip\"][i]] )\n",
    "        \n",
    "    df = pd.DataFrame(variant_info, columns=[\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\"])\n",
    "    df.to_csv(${_output[\"variants_csv\"]:r}, sep = \"\\t\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038ba60-272a-4bdf-9a6e-17141572a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate analysis report: HTML file, and optionally PPTX file\n",
    "[SuSiE_RSS_4]\n",
    "output: f\"{_input['analysis_summary']:n}.html\"\n",
    "sh: container=container_marp, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:a} \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:an}.pptx \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Singularity Susie_rpy2",
   "language": "python",
   "name": "rpy2_susie"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
